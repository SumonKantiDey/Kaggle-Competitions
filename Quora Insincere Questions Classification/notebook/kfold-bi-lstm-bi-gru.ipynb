{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Import Libraries\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import logging\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.model_selection import KFold, TimeSeriesSplit\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import gensim.models.keyedvectors as word2vec\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, CuDNNLSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D,LSTM,GRU\n",
    "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.initializers import *\n",
    "from keras.optimizers import Adam, Adadelta\n",
    "from keras.callbacks import Callback\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
    "from keras.layers import GRU, BatchNormalization, Conv1D, MaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re, string\n",
    "from nltk.tokenize import TweetTokenizer   \n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "from nltk.tokenize import word_tokenize\n",
    "lem = WordNetLemmatizer()\n",
    "tokenizer=TweetTokenizer()\n",
    "eng_stopwords = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data file\n",
    "train = pd.read_csv('../input/quora-insincere-questions-classification/train.csv')\n",
    "test = pd.read_csv('../input/quora-insincere-questions-classification/test.csv')\n",
    "submission = pd.read_csv('../input/quora-insincere-questions-classification/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00002165364db923c7e6</td>\n",
       "      <td>How did Quebec nationalists see their province...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>000032939017120e6e44</td>\n",
       "      <td>Do you have an adopted dog, how would you enco...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0000412ca6e4628ce2cf</td>\n",
       "      <td>Why does velocity affect time? Does velocity a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>000042bf85aa498cd78e</td>\n",
       "      <td>How did Otto von Guericke used the Magdeburg h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0000455dfa3e01eae3af</td>\n",
       "      <td>Can I convert montra helicon D to a mountain b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    qid                                      question_text  \\\n",
       "0  00002165364db923c7e6  How did Quebec nationalists see their province...   \n",
       "1  000032939017120e6e44  Do you have an adopted dog, how would you enco...   \n",
       "2  0000412ca6e4628ce2cf  Why does velocity affect time? Does velocity a...   \n",
       "3  000042bf85aa498cd78e  How did Otto von Guericke used the Magdeburg h...   \n",
       "4  0000455dfa3e01eae3af  Can I convert montra helicon D to a mountain b...   \n",
       "\n",
       "   target  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count the occourence of a word in whole document\n",
    "def build_vocab(texts):\n",
    "    sentences = texts.apply(lambda x: x.split()).values\n",
    "    vocab = {}\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab\n",
    "vocab_text = pd.concat([train['question_text'],test['question_text']])\n",
    "vocab = build_vocab(vocab_text)\n",
    "vocablist = list()\n",
    "for i in vocab.keys():\n",
    "    if vocab[i] > 100:\n",
    "        vocablist.append(i)\n",
    "misspellword = pd.DataFrame(vocablist)\n",
    "misspellword.to_csv('misspellword.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "misspell_dict = {'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', \n",
    "                'counselling': 'counseling', 'theatre': 'theater','cancelled': 'canceled', 'labour': 'labor', \n",
    "                'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ', \n",
    "                'Qoura': 'Quora', 'sallary': 'salary','Whta': 'What', 'wht':'what', 'whatis':'what is','narcisist': 'narcissist',\n",
    "                'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many',\n",
    "                'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', 'mastrubation': 'masturbation', \n",
    "                'banglore': 'bengaluru','mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis', 'Etherium': 'Ethereum', \n",
    "                'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota','obamacare': 'obama care', \n",
    "                'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', \n",
    "                'demonitisation': 'demonetization', 'demonitization': 'demonetization', 'demonetisation': 'demonetization',\n",
    "                'cannot': 'can not','doesnt': 'does not','ignou': 'ignore','aiims': 'aims','barack': 'barack obama','jong-un': 'kim jong un',\n",
    "                 'atleast': 'at least','iam': 'i am','ww2': 'world war 2','ww1': 'world war 1', 'arya': 'area', 'diarrhea': 'diarrhoea', 'po': 'pop'\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPROCESSING PART\n",
    "fill = {\n",
    "        \"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\",\"can't've\": \"cannot have\", \"'cause\": \"because\", \"could've\": \"could have\", \n",
    "        \"couldn't\": \"could not\", \"couldn't've\": \"could not have\",\"didn't\": \"did not\",\"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \n",
    "        \"hadn't've\": \"had not have\",\"iâ€™ve\":\"i have\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\"he'd\": \"he would\", \"he'd've\": \"he would have\", \"he'll\": \"he will\",\n",
    "        \"he'll've\": \"he he will have\", \"he's\": \"he is\", \"how'd\": \"how did\",\"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\", \n",
    "        \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\",\"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\",\"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\", \n",
    "        \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \n",
    "        \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\",\"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \n",
    "        \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\",\"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \n",
    "        \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\n",
    "        \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \n",
    "        \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\",\"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \n",
    "        \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\",\"that's\": \"that is\",\"there'd\": \"there would\", \"there'd've\": \"there would have\",\"there's\": \"there is\", \n",
    "        \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\",\"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \n",
    "        \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\",\"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \n",
    "        \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\",\"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\", \n",
    "        \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\",\"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \n",
    "        \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\",\"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \n",
    "        \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\",\"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\",\"wouldn't've\": \"would not have\",\n",
    "        \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\",\n",
    "         \"you'd've\": \"you would have\", \"you'll\": \"you will\",\"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\",\n",
    "         \"children's\": \"childrens\",\"women's\":\"womens\",\"jehovah's\":\"jehovahs\",\"girlfriend's\":\"girlfriends\",\"girl's\":\"girls\",\"sister's\": \"sisters\",\n",
    "          \"obama's\":\"obamas\",\"trumpâ€™s\":\"trumps\",\"earth's\":\"earths\",\"modi's\":\"modis\",\"google's\":\"google\",\"mcdonald's\":\"mcdonalds\"\n",
    "      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):    \n",
    "    #fixing apostrope\n",
    "    text = text.replace(\"â€™\", \"'\")\n",
    "    #to lower\n",
    "    text = text.lower() #convert all of the question text to lower case\n",
    "    #remove \\n\n",
    "    text = re.sub(\"\\\\n\",\"\",text)\n",
    "    #remove url\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    \n",
    "    #Split the sentences into words\n",
    "    words = tokenizer.tokenize(text)\n",
    "    words = [fill[word] if word in fill else word for word in words]\n",
    "    words = [misspell_dict[word] if word in misspell_dict else word for word in words]\n",
    "    #words = [lem.lemmatize(word, \"v\") for word in words]\n",
    "    words = [w for w in words if not w in eng_stopwords]\n",
    "    text = \" \".join(words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the text\n",
    "train[\"question_text\"] = train[\"question_text\"].apply(lambda x: clean_text(x))\n",
    "test[\"question_text\"] = test[\"question_text\"].apply(lambda x: clean_text(x))\n",
    "#train['word_count'] = train['question_text'].apply(lambda x: len(str(x).split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## some config values \n",
    "embed_size = 300 # how big is each word vector\n",
    "max_features = 231806# how many unique words to use\n",
    "maxlen = 72 # max number of words in a question to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill the missing value\n",
    "X_train = train[\"question_text\"].fillna(\"fillna\").values\n",
    "y_train = train[\"target\"].values\n",
    "X_test = test[\"question_text\"].fillna(\"fillna\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentence tokenization\n",
    "tokenizer = Tokenizer(num_words=max_features,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "tokenizer.fit_on_texts(list(X_train) + list(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 231806 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_para(word_index):\n",
    "    EMBEDDING_FILE = '../input/quora-insincere-questions-classification/embeddings/paragram_300_sl999/paragram_300_sl999.txt'\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE, encoding=\"utf8\", errors='ignore') if len(o)>100)\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "    word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))#distribute the value with mean and deviation\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "    del embeddings_index\n",
    "    gc.collect()\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove(word_index):\n",
    "    EMBEDDING_FILE = '../input/quora-insincere-questions-classification/embeddings/glove.840B.300d/glove.840B.300d.txt'\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std() #take the mean and std of embedding matrix\n",
    "    embed_size = all_embs.shape[1]\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))#distribute the value with mean and deviation\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "    del embeddings_index\n",
    "    gc.collect()\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:5: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix_1 = load_glove(word_index)\n",
    "embedding_matrix_2 = load_para(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(231806, 300)\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.mean((embedding_matrix_1, embedding_matrix_2), axis=0)\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "x_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(X_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_DIM = 72\n",
    "class Attention(Layer):\n",
    "\n",
    "    def __init__(self, regularizer=regularizers.l2(1e-10), **kwargs):\n",
    "        self.regularizer = regularizer\n",
    "        self.supports_masking = True\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3        \n",
    "        self.W = self.add_weight(name='W',\n",
    "                                 shape=(input_shape[-1], CONTEXT_DIM),\n",
    "                                 initializer='normal',\n",
    "                                 trainable=True, \n",
    "                                 regularizer=self.regularizer)\n",
    "        self.b = self.add_weight(name='b',\n",
    "                                 shape=(CONTEXT_DIM,),\n",
    "                                 initializer='normal',\n",
    "                                 trainable=True, \n",
    "                                 regularizer=self.regularizer)\n",
    "        self.u = self.add_weight(name='u',\n",
    "                                 shape=(CONTEXT_DIM,),\n",
    "                                 initializer='normal',\n",
    "                                 trainable=True, \n",
    "                                 regularizer=self.regularizer)        \n",
    "        super(Attention, self).build(input_shape)\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax(x, dim):\n",
    "        \"\"\"Computes softmax along a specified dim. Keras currently lacks this feature.\n",
    "        \"\"\"\n",
    "        if K.backend() == 'tensorflow':\n",
    "            import tensorflow as tf\n",
    "            return tf.nn.softmax(x, dim)\n",
    "        else:\n",
    "            raise ValueError(\"Backend '{}' not supported\".format(K.backend()))\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        ut = K.tanh(K.bias_add(K.dot(x, self.W), self.b)) * self.u\n",
    "\n",
    "        # Collapse `attention_dims` to 1. This indicates the weight for each time_step.\n",
    "        ut = K.sum(ut, axis=-1, keepdims=True)\n",
    "\n",
    "        # Convert those weights into a distribution but along time axis.\n",
    "        # i.e., sum of alphas along `time_steps` axis should be 1.\n",
    "        self.at = self.softmax(ut, dim=1)\n",
    "        if mask is not None:\n",
    "            self.at *= K.cast(K.expand_dims(mask, -1), K.floatx())\n",
    "\n",
    "        # Weighted sum along `time_steps` axis.\n",
    "        return K.sum(x * self.at, axis=-2)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {}\n",
    "        base_config = super(Attention, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def compute_mask(self, inputs, mask):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(lr = 0.0, lr_d = 0.0, units = [128,64],filters=[128,64,32], dr = 0.0):\n",
    "    inp = Input(shape=(maxlen,))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n",
    "    x = Conv1D(filters=filters[0], kernel_size = 3, kernel_initializer='he_normal',activation='relu',padding = \"valid\")(x)\n",
    "    x = Conv1D(filters=filters[1], kernel_size = 3,kernel_initializer='he_normal',activation='relu',padding = \"valid\")(x)\n",
    "    x = Bidirectional(CuDNNGRU(units[0], return_sequences=True))(x)\n",
    "    x = Bidirectional(CuDNNGRU(units[1], return_sequences=True))(x)\n",
    "    x = Attention()(x)\n",
    "    x = Dense(filters[2], kernel_initializer='he_normal',activation=\"relu\")(x)\n",
    "    x = Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(lr = lr, decay = lr_d), metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print(\"\\n ROC-AUC - epoch: {:d} - score: {:.6f}\".format(epoch+1, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_vs_thresh(y_pred,y_val):\n",
    "    thresholds = []\n",
    "    for thresh in np.arange(0.1, 0.501, 0.01):\n",
    "        thresh = np.round(thresh, 2)\n",
    "        res = metrics.f1_score(y_val, (y_pred > thresh).astype(int))\n",
    "        thresholds.append([thresh, res])\n",
    "        print(\"F1 score at threshold {0} is {1}\".format(thresh, res))\n",
    "\n",
    "    thresholds.sort(key=lambda x: x[1], reverse=True)\n",
    "    best_thresh = thresholds[0][0]\n",
    "    print(\"Best threshold: \", best_thresh)\n",
    "    return best_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "NFOLDS = 5\n",
    "folds = KFold(n_splits=NFOLDS,random_state=10, shuffle=True)\n",
    "foldscore = []\n",
    "splits = folds.split(x_train, y_train)\n",
    "y_test = np.zeros(x_test.shape[0])\n",
    "y_oof = np.zeros(x_train.shape[0])\n",
    "epochs = [8, 7, 6, 5, 4]\n",
    "val_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD1\n",
      "Build Model\n",
      "Train on 1044897 samples, validate on 261225 samples\n",
      "Epoch 1/8\n",
      "1044897/1044897 [==============================] - 107s 102us/step - loss: 0.1218 - accuracy: 0.9518 - val_loss: 0.1134 - val_accuracy: 0.9551\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.953860\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.11337, saving model to best_model.hdf5\n",
      "Epoch 2/8\n",
      "1044897/1044897 [==============================] - 104s 100us/step - loss: 0.1077 - accuracy: 0.9568 - val_loss: 0.1117 - val_accuracy: 0.9559\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.956995\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.11337 to 0.11174, saving model to best_model.hdf5\n",
      "Epoch 3/8\n",
      "1044897/1044897 [==============================] - 103s 99us/step - loss: 0.1007 - accuracy: 0.9594 - val_loss: 0.1130 - val_accuracy: 0.9567\n",
      "\n",
      " ROC-AUC - epoch: 3 - score: 0.957341\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.11174\n",
      "Epoch 4/8\n",
      "1044897/1044897 [==============================] - 103s 99us/step - loss: 0.0940 - accuracy: 0.9617 - val_loss: 0.1115 - val_accuracy: 0.9560\n",
      "\n",
      " ROC-AUC - epoch: 4 - score: 0.956099\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.11174 to 0.11147, saving model to best_model.hdf5\n",
      "Epoch 5/8\n",
      "1044897/1044897 [==============================] - 103s 99us/step - loss: 0.0874 - accuracy: 0.9640 - val_loss: 0.1136 - val_accuracy: 0.9552\n",
      "\n",
      " ROC-AUC - epoch: 5 - score: 0.955284\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.11147\n",
      "Epoch 6/8\n",
      "1044897/1044897 [==============================] - 104s 99us/step - loss: 0.0809 - accuracy: 0.9666 - val_loss: 0.1196 - val_accuracy: 0.9534\n",
      "\n",
      " ROC-AUC - epoch: 6 - score: 0.952718\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.11147\n",
      "Epoch 7/8\n",
      "1044897/1044897 [==============================] - 103s 99us/step - loss: 0.0749 - accuracy: 0.9694 - val_loss: 0.1266 - val_accuracy: 0.9535\n",
      "\n",
      " ROC-AUC - epoch: 7 - score: 0.949871\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.11147\n",
      "Epoch 8/8\n",
      "1044897/1044897 [==============================] - 104s 99us/step - loss: 0.0689 - accuracy: 0.9718 - val_loss: 0.1300 - val_accuracy: 0.9525\n",
      "\n",
      " ROC-AUC - epoch: 8 - score: 0.949049\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.11147\n",
      "train logloss:[0.12177017602327689, 0.10765353642405655, 0.10066296906057805, 0.09400566388367491, 0.08740546053369753, 0.08086781272846512, 0.07490520791754107, 0.06893082225543949]\n",
      "val logloss:[0.11337286032869641, 0.1117364863507861, 0.11303575428482082, 0.11146812371830471, 0.11357734442681305, 0.11962004734735328, 0.12660437907553343, 0.1299779893180516]\n",
      "F1 score at threshold 0.1 is 0.5868896534439153\n",
      "F1 score at threshold 0.11 is 0.5915134108491222\n",
      "F1 score at threshold 0.12 is 0.5951379917792132\n",
      "F1 score at threshold 0.13 is 0.5978586723768737\n",
      "F1 score at threshold 0.14 is 0.5999807266069191\n",
      "F1 score at threshold 0.15 is 0.6025706690080729\n",
      "F1 score at threshold 0.16 is 0.6047039660406229\n",
      "F1 score at threshold 0.17 is 0.6061482460295695\n",
      "F1 score at threshold 0.18 is 0.608038682381384\n",
      "F1 score at threshold 0.19 is 0.6096468912180727\n",
      "F1 score at threshold 0.2 is 0.6108070608186232\n",
      "F1 score at threshold 0.21 is 0.6127155620195305\n",
      "F1 score at threshold 0.22 is 0.6140943994548838\n",
      "F1 score at threshold 0.23 is 0.6158644569555678\n",
      "F1 score at threshold 0.24 is 0.6163029285581049\n",
      "F1 score at threshold 0.25 is 0.61728992048141\n",
      "F1 score at threshold 0.26 is 0.6177091795288382\n",
      "F1 score at threshold 0.27 is 0.6178533417023715\n",
      "F1 score at threshold 0.28 is 0.6182937554969217\n",
      "F1 score at threshold 0.29 is 0.6182713862674011\n",
      "F1 score at threshold 0.3 is 0.6190104747047025\n",
      "F1 score at threshold 0.31 is 0.6196060827114079\n",
      "F1 score at threshold 0.32 is 0.6195510827161191\n",
      "F1 score at threshold 0.33 is 0.6200222026130768\n",
      "F1 score at threshold 0.34 is 0.6197748044580695\n",
      "F1 score at threshold 0.35 is 0.6198540230216658\n",
      "F1 score at threshold 0.36 is 0.6195383945420235\n",
      "F1 score at threshold 0.37 is 0.61899056024783\n",
      "F1 score at threshold 0.38 is 0.6183529411764707\n",
      "F1 score at threshold 0.39 is 0.6178385416666666\n",
      "F1 score at threshold 0.4 is 0.6175568198742962\n",
      "F1 score at threshold 0.41 is 0.6167081322502324\n",
      "F1 score at threshold 0.42 is 0.6152778615820914\n",
      "F1 score at threshold 0.43 is 0.6142483739590299\n",
      "F1 score at threshold 0.44 is 0.6131279794646132\n",
      "F1 score at threshold 0.45 is 0.6126314494803516\n",
      "F1 score at threshold 0.46 is 0.6109959469075834\n",
      "F1 score at threshold 0.47 is 0.6102148863282466\n",
      "F1 score at threshold 0.48 is 0.6087038314416504\n",
      "F1 score at threshold 0.49 is 0.6073671916921815\n",
      "F1 score at threshold 0.5 is 0.6058101837136863\n",
      "Best threshold:  0.33\n",
      "FOLD2\n",
      "Build Model\n",
      "Train on 1044897 samples, validate on 261225 samples\n",
      "Epoch 1/7\n",
      "1044897/1044897 [==============================] - 104s 100us/step - loss: 0.1225 - accuracy: 0.9516 - val_loss: 0.1155 - val_accuracy: 0.9546\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.953487\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.11548, saving model to best_model.hdf5\n",
      "Epoch 2/7\n",
      "1044897/1044897 [==============================] - 104s 100us/step - loss: 0.1080 - accuracy: 0.9567 - val_loss: 0.1141 - val_accuracy: 0.9543\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.955749\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.11548 to 0.11409, saving model to best_model.hdf5\n",
      "Epoch 3/7\n",
      "1044897/1044897 [==============================] - 104s 99us/step - loss: 0.1010 - accuracy: 0.9590 - val_loss: 0.1120 - val_accuracy: 0.9553\n",
      "\n",
      " ROC-AUC - epoch: 3 - score: 0.956826\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.11409 to 0.11202, saving model to best_model.hdf5\n",
      "Epoch 4/7\n",
      "1044897/1044897 [==============================] - 104s 100us/step - loss: 0.0944 - accuracy: 0.9615 - val_loss: 0.1116 - val_accuracy: 0.9561\n",
      "\n",
      " ROC-AUC - epoch: 4 - score: 0.956045\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.11202 to 0.11164, saving model to best_model.hdf5\n",
      "Epoch 5/7\n",
      "1044898/1044898 [==============================] - 104s 100us/step - loss: 0.0879 - accuracy: 0.9642 - val_loss: 0.1154 - val_accuracy: 0.9531\n",
      "\n",
      " ROC-AUC - epoch: 5 - score: 0.955153\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.10941\n",
      "Epoch 6/6\n",
      "1044898/1044898 [==============================] - 104s 100us/step - loss: 0.1082 - accuracy: 0.9568 - val_loss: 0.1089 - val_accuracy: 0.9567\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.957500\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.11100 to 0.10894, saving model to best_model.hdf5\n",
      "Epoch 3/5\n",
      " 501760/1044898 [=============>................] - ETA: 49s - loss: 0.1006 - accuracy: 0.9597F1 score at threshold 0.1 is 0.5965800577392849\n",
      "F1 score at threshold 0.11 is 0.6033037195246643\n",
      "F1 score at threshold 0.12 is 0.6091535410239162\n",
      "F1 score at threshold 0.13 is 0.6149080073783285\n",
      "F1 score at threshold 0.14 is 0.6196085973938549\n",
      "F1 score at threshold 0.15 is 0.6230317848410758\n",
      "F1 score at threshold 0.16 is 0.6265281325166762\n",
      "F1 score at threshold 0.17 is 0.6296594396096383\n",
      "F1 score at threshold 0.18 is 0.6326634703662144\n",
      "F1 score at threshold 0.19 is 0.6353531175679867\n",
      "F1 score at threshold 0.2 is 0.6380181737240422\n",
      "F1 score at threshold 0.21 is 0.6396327061383721\n",
      "F1 score at threshold 0.22 is 0.6406153349649033\n",
      "F1 score at threshold 0.23 is 0.6417898260798564\n",
      "F1 score at threshold 0.24 is 0.6429437038669162\n",
      "F1 score at threshold 0.25 is 0.6434438782613539\n",
      "F1 score at threshold 0.26 is 0.6439351278297105\n",
      "F1 score at threshold 0.27 is 0.6437934571982218\n",
      "F1 score at threshold 0.28 is 0.643363903789583\n",
      "F1 score at threshold 0.29 is 0.6435127181608927\n",
      "F1 score at threshold 0.3 is 0.6441336670101574\n",
      "F1 score at threshold 0.31 is 0.6433096392350475\n",
      "F1 score at threshold 0.32 is 0.6424861131962167\n",
      "F1 score at threshold 0.33 is 0.6418127768003398\n",
      "F1 score at threshold 0.34 is 0.6404656149486904\n",
      "F1 score at threshold 0.35 is 0.6396243319225184\n",
      "F1 score at threshold 0.36 is 0.6388126714891494\n",
      "F1 score at threshold 0.37 is 0.6372505822370492\n",
      "F1 score at threshold 0.38 is 0.634707696220237\n",
      "F1 score at threshold 0.39 is 0.6335407711554499\n",
      "F1 score at threshold 0.4 is 0.6307981342316662\n",
      "F1 score at threshold 0.41 is 0.6282465941389787\n",
      "F1 score at threshold 0.42 is 0.6253833217924621\n",
      "F1 score at threshold 0.43 is 0.6229453650096493\n",
      "F1 score at threshold 0.44 is 0.6200369189461319\n",
      "F1 score at threshold 0.45 is 0.6169744197865493\n",
      "F1 score at threshold 0.46 is 0.6146224803553125\n",
      "F1 score at threshold 0.47 is 0.611530376649781\n",
      "F1 score at threshold 0.48 is 0.6089768104856934\n",
      "F1 score at threshold 0.49 is 0.6048078610282506\n",
      "F1 score at threshold 0.5 is 0.6014510706069722\n",
      "Best threshold:  0.3\n",
      "FOLD5\n",
      "Build Model\n",
      "Train on 1044898 samples, validate on 261224 samples\n",
      "Epoch 1/4\n",
      " 832000/1044898 [======================>.......] - ETA: 19s - loss: 0.0935 - accuracy: 0.9624"
     ]
    }
   ],
   "source": [
    "for fold_n, (train_index, valid_index) in enumerate(splits):\n",
    "    val_list += list(valid_index)\n",
    "    print('FOLD%s'%(fold_n+1))\n",
    "    x_tra, x_val, y_tra, y_val = x_train[train_index], x_train[valid_index], y_train[train_index], y_train[valid_index]\n",
    "    file_path = \"best_model.hdf5\"\n",
    "    check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n",
    "                                  save_best_only = True, mode = \"min\")\n",
    "    ra_val = RocAucEvaluation(validation_data=(x_val, y_val), interval = 1)\n",
    "    early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 5)\n",
    "    callbacks = [ra_val, check_point, early_stop]\n",
    "    model = build_model(lr = 1e-3, lr_d = 0, units = [128,64],filters=[128,64,32],dr = 0.2)\n",
    "    print('Build Model')\n",
    "    model.fit(x_tra, y_tra, batch_size=512, epochs=epochs[fold_n], \n",
    "              validation_data=(x_val, y_val), verbose=1, callbacks=callbacks \n",
    "             )\n",
    "    print(\"train logloss:%s\"%model.history.history['loss'])\n",
    "    print(\"val logloss:%s\"%model.history.history['val_loss'])\n",
    "    y_pred = model.predict([x_val], batch_size=1024, verbose=2)\n",
    "    y_test += np.squeeze(model.predict([x_test], batch_size = 1024, verbose = 2))/5\n",
    "    y_oof[valid_index] = np.squeeze(y_pred)\n",
    "    threshold = f1_vs_thresh(np.squeeze(y_pred),np.squeeze(y_val))\n",
    "    foldscore.append(threshold)\n",
    "pred_thresh = f1_vs_thresh(np.squeeze(y_oof[val_list]),np.squeeze(y_train[val_list]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred_thresh =  0.3\n"
     ]
    }
   ],
   "source": [
    "print(\"Pred_thresh = \",pred_thresh)\n",
    "y_test = y_test.reshape((-1, 1)) \n",
    "pred_test_y = (y_test>pred_thresh).astype(int) \n",
    "submission['prediction'] = pred_test_y \n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
