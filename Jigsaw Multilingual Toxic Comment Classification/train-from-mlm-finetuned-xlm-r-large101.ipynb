{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Prerequisite and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "c4ad26dd-8d2a-43e9-a3ea-0a4342a16e52",
    "_uuid": "07d7bc95-0377-4db3-aa05-2bd6cf989e78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 192 \n",
    "DROPOUT = 0.3 # use aggressive dropout\n",
    "BATCH_SIZE = 16 # per TPU core\n",
    "TOTAL_STEPS_STAGE1 = 2000\n",
    "VALIDATE_EVERY_STAGE1 = 200\n",
    "TOTAL_STEPS_STAGE2 = 200\n",
    "VALIDATE_EVERY_STAGE2 = 10\n",
    "\n",
    "### Different learning rate for transformer and head ###\n",
    "LR_TRANSFORMER = 5e-6\n",
    "LR_HEAD = 1e-3\n",
    "\n",
    "PRETRAINED_TOKENIZER=  'jplu/tf-xlm-roberta-large'\n",
    "PRETRAINED_MODEL = '/kaggle/input/jigsaw-mlm-finetuned-xlm-r-large'\n",
    "D = '/kaggle/input/jigsaw-multilingual-toxic-comment-classification/'\n",
    "D_TRANS = '/kaggle/input/jigsaw-train-multilingual-coments-google-api/'\n",
    "D_BIAS = '../input/translated-train-bias-all-langs/All languages/'\n",
    "import gc\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import transformers\n",
    "from transformers import TFRobertaModel, AutoTokenizer\n",
    "import logging\n",
    "# no extensive logging \n",
    "logging.getLogger().setLevel(logging.NOTSET)\n",
    "\n",
    "AUTO = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "dda6b430-ea9c-4772-8bd1-0938cfd6e63b",
    "_uuid": "f0c9b5d5-e529-441d-8f37-7f64a1daccb9"
   },
   "source": [
    "## Connect to TPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "86a303dd-5d1d-4d3f-bf28-4a02b6816e9b",
    "_uuid": "5a5fd45e-8f97-44d8-95f6-29cc8563dc3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on TPU  grpc://10.0.0.2:8470\n",
      "REPLICAS:  8\n"
     ]
    }
   ],
   "source": [
    "def connect_to_TPU():\n",
    "    \"\"\"Detect hardware, return appropriate distribution strategy\"\"\"\n",
    "    try:\n",
    "        # TPU detection. No parameters necessary if TPU_NAME environment variable is\n",
    "        # set: this is always the case on Kaggle.\n",
    "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "        print('Running on TPU ', tpu.master())\n",
    "    except ValueError:\n",
    "        tpu = None\n",
    "\n",
    "    if tpu:\n",
    "        tf.config.experimental_connect_to_cluster(tpu)\n",
    "        tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "    else:\n",
    "        # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "        strategy = tf.distribute.get_strategy()\n",
    "\n",
    "    global_batch_size = BATCH_SIZE * strategy.num_replicas_in_sync\n",
    "\n",
    "    return tpu, strategy, global_batch_size\n",
    "\n",
    "\n",
    "tpu, strategy, global_batch_size = connect_to_TPU()\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9e748f90-73ca-4a3e-a3a5-aef269ce89c4",
    "_uuid": "2111bb0e-44ed-4db7-aba8-d2ff3507c454"
   },
   "source": [
    " ## Load text data into memory\n",
    " \n",
    " - Traning data is englih + all translations. The reason to use english too is that people use english in their foreign language comments all the time.\n",
    " - Not using the full dataset, downsampling negatives to 50-50%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "1cba2ff3-145e-4187-a6a4-60d28adf5141",
    "_uuid": "80b620f4-85cd-4436-b868-acdcea7bf2b6"
   },
   "outputs": [],
   "source": [
    "def load_jigsaw_trans(langs=['tr','it','es','ru','fr','pt'], \n",
    "                      columns=['comment_text', 'toxic']):\n",
    "    train_6langs=[]\n",
    "    for i in range(len(langs)):\n",
    "        fn = D_TRANS+'jigsaw-toxic-comment-train-google-%s-cleaned.csv'%langs[i]\n",
    "        train_6langs.append(downsample(pd.read_csv(fn)[columns]))\n",
    "\n",
    "    return train_6langs\n",
    "\n",
    "def downsample(df):\n",
    "    \"\"\"Subsample the train dataframe to 50%-50%\"\"\"\n",
    "    ds_df= pd.concat([\n",
    "        df.query('toxic==1'),\n",
    "        df.query('toxic==0').sample(sum(df.toxic))\n",
    "    ])\n",
    "    \n",
    "    return ds_df\n",
    "    \n",
    "\n",
    "train_df = pd.concat(load_jigsaw_trans()) \n",
    "val_df = pd.read_csv(D+'validation.csv')\n",
    "test_df = pd.read_csv(D+'test.csv')\n",
    "sub_df = pd.read_csv(D+'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256496"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "external = '/kaggle/input/toxic-comment-detection-multilingual-extended/archive/'\n",
    "e_ru = pd.read_csv(f'{external}russian/labeled.csv')\n",
    "\n",
    "e_tr = pd.read_csv(f'{external}turkish/troff-v1.0.tsv', sep='\\t', header=0)\n",
    "e_tr.label = e_tr.label.apply(lambda x: 1 if x not in ['non', 'prof'] else 0)\n",
    "\n",
    "e_it = pd.concat([\n",
    "    pd.read_csv(f'{external}italian/haspeede_FB-train.tsv', sep='\\t', header=0),\n",
    "    pd.read_csv(f'{external}italian/haspeede_TW-train.tsv', sep='\\t', header=0)\n",
    "])\n",
    "e_ru.rename(columns={'comment':'comment_text'}, inplace=True)\n",
    "e_tr.rename(columns={'text':'comment_text', 'label':'toxic'}, inplace=True)\n",
    "e_it.rename(columns={'comment':'comment_text'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Toxic trans_bias =  120000\n",
      "Train Intoxic trans_bias =  120000\n"
     ]
    }
   ],
   "source": [
    "trans_bias=pd.DataFrame()\n",
    "for file in os.listdir(D_BIAS):\n",
    "    if file.startswith(\"train-bias-toxic-google-api\") and file.endswith(\"cleaned.csv\"):\n",
    "        df = pd.read_csv(D_BIAS+file,usecols=['comment_text','toxic'])\n",
    "        df.toxic = df.toxic.round().astype(int)\n",
    "        toxic = df[['comment_text', 'toxic']].query('toxic==1').sample(20000,random_state=42)\n",
    "        in_toxic = df[['comment_text', 'toxic']].query('toxic==0').sample(20000,random_state=42)\n",
    "        trans_bias=trans_bias.append(pd.concat([in_toxic,toxic]))\n",
    "#In train\n",
    "print(\"Train Toxic trans_bias = \",len(trans_bias[['comment_text', 'toxic']].query('toxic==1')))\n",
    "print(\"Train Intoxic trans_bias = \",len(trans_bias[['comment_text', 'toxic']].query('toxic==0')))\n",
    "trans_bias = trans_bias.drop_duplicates(subset='comment_text')\n",
    "trans_bias = trans_bias.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.concat([train_df,trans_bias,e_ru,e_tr,e_it],axis = 0).reset_index(drop=True)\n",
    "train_df = train_df.drop_duplicates(subset='comment_text')\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "train_df = train_df.sample(frac=1).reset_index(drop=True)\n",
    "del e_ru,e_tr,e_it,trans_bias\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d070fa12-8165-41c2-9828-9690c8efa826",
    "_uuid": "5e6d306f-f2ca-4452-9ace-661e6df8bae8"
   },
   "source": [
    "## Tokenize  it with the models own tokenizer\n",
    "\n",
    "- Note it takes some time ( approx 5 minutes)\n",
    "- Note, we need to reshape the targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "900f2d01-b87c-42e7-8cc2-8dcba95ffa54",
    "_uuid": "7d5605a3-df77-4574-9899-89529d0a6e61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 31s, sys: 2.54 s, total: 8min 34s\n",
      "Wall time: 8min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def regular_encode(texts, tokenizer, maxlen=512):\n",
    "    enc_di = tokenizer.batch_encode_plus(\n",
    "        texts, \n",
    "        return_attention_masks=False, \n",
    "        return_token_type_ids=False,\n",
    "        pad_to_max_length=True,\n",
    "        max_length=maxlen\n",
    "    )\n",
    "    \n",
    "    return np.array(enc_di['input_ids'])\n",
    "    \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_TOKENIZER)\n",
    "X_train = regular_encode(train_df.comment_text.values, tokenizer, maxlen=MAX_LEN)\n",
    "X_val = regular_encode(val_df.comment_text.values, tokenizer, maxlen=MAX_LEN)\n",
    "X_test = regular_encode(test_df.content.values, tokenizer, maxlen=MAX_LEN)\n",
    "\n",
    "y_train = train_df.toxic.values.reshape(-1,1)\n",
    "y_val = val_df.toxic.values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0725dd72-142e-4139-8e12-103302d53bc9",
    "_uuid": "13500d69-d19f-4ca0-8958-d4d8af5610e8"
   },
   "source": [
    "## Create distributed tensorflow datasets\n",
    "\n",
    "- Note, validation dataset does not contain labels, we keep track of it ourselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "757b2c93-9c07-441d-ba2e-47b8607b3795",
    "_uuid": "8b748024-314d-4256-b67b-0bc71ecac19d"
   },
   "outputs": [],
   "source": [
    "def create_dist_dataset(X, y=None, training=False):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "\n",
    "    ### Add y if present ###\n",
    "    if y is not None:\n",
    "        dataset_y = tf.data.Dataset.from_tensor_slices(y)\n",
    "        dataset = tf.data.Dataset.zip((dataset, dataset_y))\n",
    "        \n",
    "    ### Repeat if training ###\n",
    "    if training:\n",
    "        dataset = dataset.shuffle(len(X)).repeat()\n",
    "\n",
    "    dataset = dataset.batch(global_batch_size).prefetch(AUTO)\n",
    "\n",
    "    ### make it distributed  ###\n",
    "    dist_dataset = strategy.experimental_distribute_dataset(dataset)\n",
    "\n",
    "    return dist_dataset\n",
    "    \n",
    "    \n",
    "train_dist_dataset = create_dist_dataset(X_train, y_train, True)\n",
    "val_dist_dataset   = create_dist_dataset(X_val)\n",
    "test_dist_dataset  = create_dist_dataset(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "bcfe8f7d-cb29-414b-9858-b36dc57a9af1",
    "_uuid": "7ae0b584-8ec8-4e10-928b-6c05ea2ea515"
   },
   "source": [
    "## Build model from pretrained transformer\n",
    "The reasoning is the following, the transformer is trained for super long time and has a very good multilingual representaton, which we only want to change a little, while the head needs to be trained from scratch.\n",
    "\n",
    "We define 2 separate optimizers for the transofmer and the head layer. This is a simple way to use different learning rate for the transformer and the head. The caffe style \"lr_multiplier\" option would be more elegant but that is not available in keras.\n",
    "\n",
    "We add the name 'custom' to the head layer, so that we can find it later and use a different learning rate with this layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "3ba0b8c0-3e4c-4afe-8766-844c6b6a8c0f",
    "_uuid": "9f6161c2-a256-4285-9a0e-f23700f4b195"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_word_ids (InputLayer)  [(None, 192)]             0         \n",
      "_________________________________________________________________\n",
      "tf_roberta_model (TFRobertaM ((None, 192, 1024), (None 559890432 \n",
      "_________________________________________________________________\n",
      "tf_op_layer_strided_slice (T [(None, 1024)]            0         \n",
      "_________________________________________________________________\n",
      "dropout_74 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "custom_head (Dense)          (None, 1)                 1025      \n",
      "=================================================================\n",
      "Total params: 559,891,457\n",
      "Trainable params: 559,891,457\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "CPU times: user 38.4 s, sys: 34 s, total: 1min 12s\n",
      "Wall time: 1min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from tensorflow.keras.layers import  Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D,Average,Dropout,concatenate\n",
    "def create_model_and_optimizer():\n",
    "    with strategy.scope():\n",
    "        transformer_layer = TFRobertaModel.from_pretrained(PRETRAINED_MODEL)                \n",
    "        model = build_model1(transformer_layer)\n",
    "        optimizer_transformer = Adam(learning_rate=LR_TRANSFORMER)\n",
    "        optimizer_head = Adam(learning_rate=LR_HEAD)\n",
    "    return model, optimizer_transformer, optimizer_head\n",
    "\n",
    "\n",
    "def build_model1(transformer):\n",
    "    inp = Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    # Huggingface transformers have multiple outputs, embeddings are the first one\n",
    "    # let's slice out the first position, the paper says its not worse than pooling\n",
    "    x = transformer(inp)[0][:, 0, :]  \n",
    "    x = Dropout(DROPOUT)(x)\n",
    "    ### note, adding the name to later identify these weights for different LR\n",
    "    out = Dense(1, activation='sigmoid', name='custom_head')(x)\n",
    "    model = Model(inputs=[inp], outputs=[out])\n",
    "    \n",
    "    return model\n",
    "def build_model2(transformer):\n",
    "    input_word_ids = Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    sequence_output = transformer(input_word_ids)[0]\n",
    "    apool= GlobalAveragePooling1D()(sequence_output)\n",
    "    mpol = GlobalMaxPooling1D()(sequence_output)\n",
    "    cat = concatenate([\n",
    "        apool,\n",
    "        mpol\n",
    "    ])\n",
    "    x = Dropout(DROPOUT)(cat)\n",
    "    out = Dense(1, activation='sigmoid',name='custom_head')(x)\n",
    "    model = Model(inputs=[input_word_ids], outputs=[out])\n",
    "\n",
    "    return model\n",
    "\n",
    "def build_model3(transformer):\n",
    "    input_word_ids = Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    seq_out = transformer(input_word_ids)[0]\n",
    "    pool= GlobalAveragePooling1D()(seq_out)\n",
    "    \n",
    "    dense=[]\n",
    "    FC = Dense(32,activation='relu')\n",
    "    for p in np.linspace(0.2,0.5,3):\n",
    "        x=Dropout(p)(pool)\n",
    "        x=FC(x)\n",
    "        x=Dense(1,activation='sigmoid')(x)\n",
    "        dense.append(x)\n",
    "    \n",
    "    out = Average()(dense)\n",
    "    model = Model(inputs=[input_word_ids], outputs=[out])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "model, optimizer_transformer, optimizer_head = create_model_and_optimizer()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0e02e579-4010-4a52-b4ba-f7157b5c9f42",
    "_uuid": "9ce1d72f-d16c-4cde-9edc-d63926623155"
   },
   "source": [
    "### Define stuff for the custom training loop\n",
    "\n",
    "We will need:\n",
    "- 1, losses, and  optionally a training AUC metric here: these need to be defined in the scope of the distributed strategy. \n",
    "- 2, A full training loop\n",
    "- 3, A distributed train step called in the training loop, which uses a single replica train step\n",
    "- 4, A prediction loop with dstibute \n",
    "\n",
    "\n",
    "At the end of training we restore the parameters which had the best validation score.\n",
    "\n",
    "\n",
    "For the different learning rate we need to apply gradients in two steps, check the train_step function for details.\n",
    "\n",
    "\n",
    "\n",
    "- Note, we are using exact AUC, for the valdationdata, and approximate AUC for the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "20b20e6a-16c4-475f-92ef-2a7809d21621",
    "_uuid": "50c766ac-68bd-4884-ba54-657f5d385786"
   },
   "outputs": [],
   "source": [
    "def define_losses_and_metrics():\n",
    "    with strategy.scope():\n",
    "        loss_object = tf.keras.losses.BinaryCrossentropy(\n",
    "            reduction=tf.keras.losses.Reduction.NONE, from_logits=False)\n",
    "\n",
    "        def compute_loss(labels, predictions):\n",
    "            per_example_loss = loss_object(labels, predictions)\n",
    "            loss = tf.nn.compute_average_loss(\n",
    "                per_example_loss, global_batch_size = global_batch_size)\n",
    "            return loss\n",
    "\n",
    "        train_accuracy_metric = tf.keras.metrics.AUC(name='training_AUC')\n",
    "\n",
    "    return compute_loss, train_accuracy_metric\n",
    "\n",
    "\n",
    "def train(train_dist_dataset, val_dist_dataset=None, y_val=None,\n",
    "          total_steps=2000, validate_every=200):\n",
    "    best_weights, history = None, []\n",
    "    step = 0\n",
    "    ### Training lopp ###\n",
    "    for tensor in train_dist_dataset:\n",
    "        distributed_train_step(tensor) \n",
    "        step+=1\n",
    "\n",
    "        if (step % validate_every == 0):   \n",
    "            ### Print train metrics ###  \n",
    "            train_metric = train_accuracy_metric.result().numpy()\n",
    "            print(\"Step %d, train AUC: %.5f\" % (step, train_metric))   \n",
    "            \n",
    "            ### Test loop with exact AUC ###\n",
    "            if val_dist_dataset:\n",
    "                val_metric = roc_auc_score(y_val, predict(val_dist_dataset))\n",
    "                print(\"Step %d,   val AUC: %.5f\" %  (step,val_metric))   \n",
    "                \n",
    "                # save weights if it is the best yet\n",
    "                history.append(val_metric)\n",
    "                if history[-1] == max(history):\n",
    "                    best_weights = model.get_weights()\n",
    "\n",
    "            ### Reset (train) metrics ###\n",
    "            train_accuracy_metric.reset_states()\n",
    "            \n",
    "        if step  == total_steps:\n",
    "            break\n",
    "    \n",
    "    ### Restore best weighths ###\n",
    "    model.set_weights(best_weights)\n",
    "\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def distributed_train_step(data):\n",
    "    strategy.experimental_run_v2(train_step, args=(data,))\n",
    "\n",
    "def train_step(inputs):\n",
    "    features, labels = inputs\n",
    "    \n",
    "    ### get transformer and head separate vars\n",
    "    # get rid of pooler head with None gradients\n",
    "    transformer_trainable_variables = [ v for v in model.trainable_variables \n",
    "                                       if (('pooler' not in v.name)  and \n",
    "                                           ('custom' not in v.name))]\n",
    "    head_trainable_variables = [ v for v in model.trainable_variables \n",
    "                                if 'custom'  in v.name]\n",
    "\n",
    "    # calculate the 2 gradients ( note persistent, and del)\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        predictions = model(features, training=True)\n",
    "        loss = compute_loss(labels, predictions)\n",
    "    gradients_transformer = tape.gradient(loss, transformer_trainable_variables)\n",
    "    gradients_head = tape.gradient(loss, head_trainable_variables)\n",
    "    del tape\n",
    "        \n",
    "    ### make the 2 gradients steps\n",
    "    optimizer_transformer.apply_gradients(zip(gradients_transformer, \n",
    "                                              transformer_trainable_variables))\n",
    "    optimizer_head.apply_gradients(zip(gradients_head, \n",
    "                                       head_trainable_variables))\n",
    "\n",
    "    train_accuracy_metric.update_state(labels, predictions)\n",
    "\n",
    "\n",
    "\n",
    "def predict(dataset):  \n",
    "    predictions = []\n",
    "    for tensor in dataset:\n",
    "        predictions.append(distributed_prediction_step(tensor))\n",
    "    ### stack replicas and batches\n",
    "    predictions = np.vstack(list(map(np.vstack,predictions)))\n",
    "    return predictions\n",
    "\n",
    "@tf.function\n",
    "def distributed_prediction_step(data):\n",
    "    predictions = strategy.experimental_run_v2(prediction_step, args=(data,))\n",
    "    return strategy.experimental_local_results(predictions)\n",
    "\n",
    "def prediction_step(inputs):\n",
    "    features = inputs  # note datasets used in prediction do not have labels\n",
    "    predictions = model(features, training=False)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "compute_loss, train_accuracy_metric = define_losses_and_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f7958337-2407-4734-98a0-51c89f4f9dc6",
    "_uuid": "e2997f2d-ae14-4bd3-b109-82e6700abb33"
   },
   "source": [
    "## Finally train it on english comments\n",
    "\n",
    "\n",
    "- Note it takes some time\n",
    "- Don't mind the warning: \"Converting sparse IndexedSlices to a dense Tensor\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "d3d0e97f-d7d4-4d3f-8c6e-925669565248",
    "_uuid": "b30a95a7-ae7e-40e8-846b-9d31bfe997ad"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:431: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 256002048 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 200, train AUC: 0.82907\n",
      "Step 200,   val AUC: 0.92894\n",
      "Step 400, train AUC: 0.93743\n",
      "Step 400,   val AUC: 0.93307\n",
      "Step 600, train AUC: 0.94271\n",
      "Step 600,   val AUC: 0.93755\n",
      "Step 800, train AUC: 0.94864\n",
      "Step 800,   val AUC: 0.93314\n",
      "Step 1000, train AUC: 0.95307\n",
      "Step 1000,   val AUC: 0.93767\n",
      "Step 1200, train AUC: 0.95232\n",
      "Step 1200,   val AUC: 0.93776\n",
      "Step 1400, train AUC: 0.95624\n",
      "Step 1400,   val AUC: 0.93518\n",
      "Step 1600, train AUC: 0.95618\n",
      "Step 1600,   val AUC: 0.93577\n",
      "Step 1800, train AUC: 0.95830\n",
      "Step 1800,   val AUC: 0.94019\n",
      "Step 2000, train AUC: 0.96155\n",
      "Step 2000,   val AUC: 0.93649\n",
      "CPU times: user 4min 16s, sys: 1min 4s, total: 5min 20s\n",
      "Wall time: 18min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train(train_dist_dataset, val_dist_dataset, y_val,\n",
    "      TOTAL_STEPS_STAGE1, VALIDATE_EVERY_STAGE1)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a280adb2-4463-4ad4-8f4a-22bc43292d82",
    "_uuid": "803c2d73-e750-4189-aa53-db248fc25ea7"
   },
   "source": [
    "## Finetune it on the validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "1bfbb20f-409d-487f-bdf0-fca60927a868",
    "_uuid": "d5b8523a-69d6-41f0-971b-8a2bfcb9e908"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:431: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 256002048 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10, train AUC: 0.92700\n",
      "Step 10,   val AUC: 0.96192\n",
      "Step 20, train AUC: 0.93180\n",
      "Step 20,   val AUC: 0.96225\n",
      "Step 30, train AUC: 0.93459\n",
      "Step 30,   val AUC: 0.96387\n",
      "Step 40, train AUC: 0.94977\n",
      "Step 40,   val AUC: 0.96437\n",
      "Step 50, train AUC: 0.94555\n",
      "Step 50,   val AUC: 0.96384\n",
      "Step 60, train AUC: 0.93536\n",
      "Step 60,   val AUC: 0.96289\n",
      "Step 70, train AUC: 0.95708\n",
      "Step 70,   val AUC: 0.96945\n",
      "Step 80, train AUC: 0.95547\n",
      "Step 80,   val AUC: 0.96506\n",
      "Step 90, train AUC: 0.94959\n",
      "Step 90,   val AUC: 0.95925\n",
      "Step 100, train AUC: 0.96000\n",
      "Step 100,   val AUC: 0.96639\n",
      "Step 110, train AUC: 0.95889\n",
      "Step 110,   val AUC: 0.96753\n",
      "Step 120, train AUC: 0.96691\n",
      "Step 120,   val AUC: 0.96820\n",
      "Step 130, train AUC: 0.97480\n",
      "Step 130,   val AUC: 0.97086\n",
      "Step 140, train AUC: 0.96716\n",
      "Step 140,   val AUC: 0.97098\n",
      "Step 150, train AUC: 0.95388\n",
      "Step 150,   val AUC: 0.96701\n",
      "Step 160, train AUC: 0.96734\n",
      "Step 160,   val AUC: 0.96219\n",
      "Step 170, train AUC: 0.96974\n",
      "Step 170,   val AUC: 0.96270\n",
      "Step 180, train AUC: 0.98154\n",
      "Step 180,   val AUC: 0.96800\n",
      "Step 190, train AUC: 0.97592\n",
      "Step 190,   val AUC: 0.96975\n",
      "Step 200, train AUC: 0.98026\n",
      "Step 200,   val AUC: 0.96975\n",
      "CPU times: user 1min 38s, sys: 42.4 s, total: 2min 21s\n",
      "Wall time: 4min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# decrease LR for second stage in the head\n",
    "optimizer_head.learning_rate.assign(1e-4)\n",
    "\n",
    "# split validation data into train test\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_val, y_val, test_size = 0.1)\n",
    "\n",
    "# make a datasets\n",
    "train_dist_dataset = create_dist_dataset(X_train, y_train, training=True)\n",
    "val_dist_dataset = create_dist_dataset(X_val, y_val)\n",
    "\n",
    "# train again\n",
    "train(train_dist_dataset, val_dist_dataset, y_val,\n",
    "      total_steps = TOTAL_STEPS_STAGE2, \n",
    "      validate_every = VALIDATE_EVERY_STAGE2)  # not validating but printing now"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e8be7da4-a25f-482c-88fa-c9b87a8a5f1e",
    "_uuid": "62874aa9-54a8-495b-a2a9-ac726c6dbecf"
   },
   "source": [
    "## Make predictions and submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "96a7be93-0dce-4d00-92c4-d4c4cb2238f8",
    "_uuid": "5cd548e4-5756-4d92-b72c-f71cb72b395d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26 s, sys: 4.68 s, total: 30.7 s\n",
      "Wall time: 1min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sub_df['toxic'] = predict(test_dist_dataset)[:,0]\n",
    "sub_df.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
