{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tqdm/std.py:666: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "#load packages\n",
    "import re\n",
    "import time\n",
    "import gc\n",
    "import random\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "from gensim.models import KeyedVectors\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "#seed everything\n",
    "def seed_torch(seed=1029):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define hyperparameters\n",
    "embed_size = 300 # how big is each word vector\n",
    "max_features = 120000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 72 # max number of words in a question to use\n",
    "\n",
    "batch_size = 1536\n",
    "train_epochs = 8\n",
    "\n",
    "SEED = 1029"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correct misspelling, remove number and punctuation\n",
    "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
    " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
    " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
    " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
    " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "\n",
    "def clean_text(x):\n",
    "    x = str(x)\n",
    "    for punct in puncts:\n",
    "        x = x.replace(punct, f' {punct} ')\n",
    "    return x\n",
    "\n",
    "def clean_numbers(x):\n",
    "    x = re.sub('[0-9]{5,}', '#####', x)\n",
    "    x = re.sub('[0-9]{4}', '####', x)\n",
    "    x = re.sub('[0-9]{3}', '###', x)\n",
    "    x = re.sub('[0-9]{2}', '##', x)\n",
    "    return x\n",
    "\n",
    "mispell_dict = {\"aren't\" : \"are not\",\n",
    "                \"can't\" : \"cannot\",\n",
    "                \"couldn't\" : \"could not\",\n",
    "                \"didn't\" : \"did not\",\n",
    "                \"doesn't\" : \"does not\",\n",
    "                \"don't\" : \"do not\",\n",
    "                \"hadn't\" : \"had not\",\n",
    "                \"hasn't\" : \"has not\",\n",
    "                \"haven't\" : \"have not\",\n",
    "                \"he'd\" : \"he would\",\n",
    "                \"he'll\" : \"he will\",\n",
    "                \"he's\" : \"he is\",\n",
    "                \"i'd\" : \"I would\",\n",
    "                \"i'd\" : \"I had\",\n",
    "                \"i'll\" : \"I will\",\n",
    "                \"i'm\" : \"I am\",\n",
    "                \"isn't\" : \"is not\",\n",
    "                \"it's\" : \"it is\",\n",
    "                \"it'll\":\"it will\",\n",
    "                \"i've\" : \"I have\",\n",
    "                \"let's\" : \"let us\",\n",
    "                \"mightn't\" : \"might not\",\n",
    "                \"mustn't\" : \"must not\",\n",
    "                \"shan't\" : \"shall not\",\n",
    "                \"she'd\" : \"she would\",\n",
    "                \"she'll\" : \"she will\",\n",
    "                \"she's\" : \"she is\",\n",
    "                \"shouldn't\" : \"should not\",\n",
    "                \"that's\" : \"that is\",\n",
    "                \"there's\" : \"there is\",\n",
    "                \"they'd\" : \"they would\",\n",
    "                \"they'll\" : \"they will\",\n",
    "                \"they're\" : \"they are\",\n",
    "                \"they've\" : \"they have\",\n",
    "                \"we'd\" : \"we would\",\n",
    "                \"we're\" : \"we are\",\n",
    "                \"weren't\" : \"were not\",\n",
    "                \"we've\" : \"we have\",\n",
    "                \"what'll\" : \"what will\",\n",
    "                \"what're\" : \"what are\",\n",
    "                \"what's\" : \"what is\",\n",
    "                \"what've\" : \"what have\",\n",
    "                \"where's\" : \"where is\",\n",
    "                \"who'd\" : \"who would\",\n",
    "                \"who'll\" : \"who will\",\n",
    "                \"who're\" : \"who are\",\n",
    "                \"who's\" : \"who is\",\n",
    "                \"who've\" : \"who have\",\n",
    "                \"won't\" : \"will not\",\n",
    "                \"wouldn't\" : \"would not\",\n",
    "                \"you'd\" : \"you would\",\n",
    "                \"you'll\" : \"you will\",\n",
    "                \"you're\" : \"you are\",\n",
    "                \"you've\" : \"you have\",\n",
    "                \"'re\": \" are\",\n",
    "                \"wasn't\": \"was not\",\n",
    "                \"we'll\":\" will\",\n",
    "                \"didn't\": \"did not\",\n",
    "                \"tryin'\":\"trying\"\n",
    "               }\n",
    "\n",
    "def _get_mispell(mispell_dict):\n",
    "    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n",
    "    return mispell_dict, mispell_re\n",
    "\n",
    "mispellings, mispellings_re = _get_mispell(mispell_dict)\n",
    "\n",
    "def replace_typical_misspell(text):\n",
    "    def replace(match):\n",
    "        return mispellings[match.group(0)]\n",
    "    return mispellings_re.sub(replace, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data file and preprocessing the data\n",
    "def load_and_prec():\n",
    "    train_df = pd.read_csv(\"../input/quora-insincere-questions-classification/train.csv\")\n",
    "    test_df = pd.read_csv(\"../input/quora-insincere-questions-classification/test.csv\")\n",
    "    print(\"Train shape : \",train_df.shape)\n",
    "    print(\"Test shape : \",test_df.shape)\n",
    "    \n",
    "    # lower\n",
    "    train_df[\"question_text\"] = train_df[\"question_text\"].progress_apply(lambda x: x.lower())\n",
    "    test_df[\"question_text\"] = test_df[\"question_text\"].progress_apply(lambda x: x.lower())\n",
    "    \n",
    "    # Clean the text\n",
    "    train_df[\"question_text\"] = train_df[\"question_text\"].progress_apply(lambda x: clean_text(x))\n",
    "    test_df[\"question_text\"] = test_df[\"question_text\"].progress_apply(lambda x: clean_text(x))\n",
    "    \n",
    "    # Clean numbers\n",
    "    train_df[\"question_text\"] = train_df[\"question_text\"].progress_apply(lambda x: clean_numbers(x))\n",
    "    test_df[\"question_text\"] = test_df[\"question_text\"].progress_apply(lambda x: clean_numbers(x))\n",
    "    \n",
    "    # Clean speelings\n",
    "    train_df[\"question_text\"] = train_df[\"question_text\"].progress_apply(lambda x: replace_typical_misspell(x))\n",
    "    test_df[\"question_text\"] = test_df[\"question_text\"].progress_apply(lambda x: replace_typical_misspell(x))\n",
    "    \n",
    "    ## fill up the missing values\n",
    "    train_X = train_df[\"question_text\"].fillna(\"_##_\").values\n",
    "    test_X = test_df[\"question_text\"].fillna(\"_##_\").values\n",
    "\n",
    "    ## Tokenize the sentences\n",
    "    tokenizer = Tokenizer(num_words=max_features)\n",
    "    tokenizer.fit_on_texts(list(train_X))\n",
    "    train_X = tokenizer.texts_to_sequences(train_X)\n",
    "    test_X = tokenizer.texts_to_sequences(test_X)\n",
    "\n",
    "    ## Pad the sentences \n",
    "    train_X = pad_sequences(train_X, maxlen=maxlen)\n",
    "    test_X = pad_sequences(test_X, maxlen=maxlen)\n",
    "\n",
    "    ## Get the target values\n",
    "    train_y = train_df['target'].values\n",
    "    \n",
    "    #shuffling the data\n",
    "    np.random.seed(SEED)\n",
    "    trn_idx = np.random.permutation(len(train_X))\n",
    "\n",
    "    train_X = train_X[trn_idx]\n",
    "    train_y = train_y[trn_idx]\n",
    "    return train_X, test_X, train_y, tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract all embedding file \n",
    "zip_ref = zipfile.ZipFile('/kaggle/input/quora-insincere-questions-classification/embeddings.zip', 'r')\n",
    "\n",
    "#print(zip_ref.namelist())\n",
    "def load_glove(word_index, max_features, unk_uni):\n",
    "    def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n",
    "    EMBEDDING_FILE = zip_ref.open('glove.840B.300d/glove.840B.300d.txt', 'r')\n",
    "    embeddings_index = dict(get_coefs(*o.decode().split(\" \"))  for o in EMBEDDING_FILE)\n",
    "\n",
    "    all_embs = np.stack(list(embeddings_index.values()))\n",
    "    emb_mean, emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    unknown_words = []\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "\n",
    "    if unk_uni:\n",
    "        embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    else:\n",
    "        embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= nb_words:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is None:\n",
    "            embedding_vector = embeddings_index.get(word.lower())\n",
    "            if embedding_vector is None:\n",
    "                unknown_words.append((word, i))\n",
    "            else:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "        else:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    print('\\nTotal unknowns glove', len(unknown_words))\n",
    "    print(unknown_words[:10])\n",
    "\n",
    "    del embeddings_index\n",
    "    gc.collect()\n",
    "    return embedding_matrix, unknown_words\n",
    "\n",
    "\n",
    "def load_wiki(word_index, max_features, unk_uni):\n",
    "    def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n",
    "    \n",
    "    EMBEDDING_FILE = zip_ref.open('wiki-news-300d-1M/wiki-news-300d-1M.vec', 'r')\n",
    "    embeddings_index = dict(get_coefs(*o.decode().split(\" \"))  for o in EMBEDDING_FILE if len(o) > 100)\n",
    "\n",
    "    all_embs = np.stack(list(embeddings_index.values()))\n",
    "    emb_mean, emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    unknown_words = []\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "\n",
    "    if unk_uni:\n",
    "        embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    else:\n",
    "        embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= nb_words:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is None:\n",
    "            embedding_vector = embeddings_index.get(word.lower())\n",
    "            if embedding_vector is None:\n",
    "                unknown_words.append((word, i))\n",
    "            else:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "        else:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    print('\\nTotal unknowns wiki', len(unknown_words))\n",
    "    print(unknown_words[:10])\n",
    "\n",
    "    del embeddings_index\n",
    "    gc.collect()\n",
    "    return embedding_matrix, unknown_words\n",
    "\n",
    "\n",
    "def load_parag(word_index, max_features, unk_uni):\n",
    "    EMBEDDING_FILE = '../input/paragram-300-sl999/paragram_300_sl999/paragram_300_sl999/paragram_300_sl999.txt'\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE, encoding=\"utf8\", errors='ignore') if len(o)>100)\n",
    "\n",
    "    all_embs = np.stack(list(embeddings_index.values()))\n",
    "    emb_mean, emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    unknown_words = []\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    if unk_uni:\n",
    "        embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    else:\n",
    "        embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= nb_words:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is None:\n",
    "            embedding_vector = embeddings_index.get(word.lower())\n",
    "            if embedding_vector is None:\n",
    "                unknown_words.append((word, i))\n",
    "            else:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "        else:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    print('\\nTotal unknowns parag', len(unknown_words))\n",
    "    print(unknown_words[:10])\n",
    "\n",
    "    del embeddings_index\n",
    "    gc.collect()\n",
    "    return embedding_matrix, unknown_words\n",
    "\n",
    "\n",
    "# https://www.kaggle.com/strideradu/word2vec-and-gensim-go-go-go\n",
    "def load_ggle(word_index, max_features, unk_uni):\n",
    "    EMBEDDING_FILE = zip_ref.open('GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin', 'r')\n",
    "    embeddings_index = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n",
    "    embed_size = embeddings_index.get_vector('known').size\n",
    "\n",
    "    unknown_words = []\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    if unk_uni:\n",
    "        embedding_matrix = (np.random.rand(nb_words, embed_size) - 0.5) / 5.0\n",
    "    else:\n",
    "        embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= nb_words:\n",
    "            continue\n",
    "        if word in embeddings_index:\n",
    "            embedding_vector = embeddings_index.get_vector(word)\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        else:\n",
    "            word_lower = word.lower()\n",
    "            if word_lower in embeddings_index:\n",
    "                embedding_matrix[i] = embeddings_index.get_vector(word_lower)\n",
    "            else:\n",
    "                unknown_words.append((word, i))\n",
    "\n",
    "    print('\\nTotal unknowns ggle', len(unknown_words))\n",
    "    print(unknown_words[:10])\n",
    "\n",
    "    del embeddings_index\n",
    "    gc.collect()\n",
    "    return embedding_matrix, unknown_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tqdm/std.py:666: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n",
      "  3%|▎         | 35747/1306122 [00:00<00:03, 357463.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape :  (1306122, 3)\n",
      "Test shape :  (375806, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1306122/1306122 [00:02<00:00, 543502.14it/s]\n",
      "100%|██████████| 375806/375806 [00:00<00:00, 407341.61it/s]\n",
      "100%|██████████| 1306122/1306122 [00:58<00:00, 22385.51it/s]\n",
      "100%|██████████| 375806/375806 [00:16<00:00, 23062.08it/s]\n",
      "100%|██████████| 1306122/1306122 [00:24<00:00, 53930.49it/s]\n",
      "100%|██████████| 375806/375806 [00:06<00:00, 56141.48it/s]\n",
      "100%|██████████| 1306122/1306122 [00:15<00:00, 83045.52it/s]\n",
      "100%|██████████| 375806/375806 [00:04<00:00, 89478.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 3.55 minutes\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "start_time = time.time()\n",
    "train_X, test_X, train_y, word_index = load_and_prec()\n",
    "total_time = (time.time() - start_time) / 60\n",
    "print(\"Took {:.2f} minutes\".format(total_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_embeddings(word_index, max_features, clean_num=False, unk_uni=True):\n",
    "    if clean_num == 2:\n",
    "        ggle_word_index = {}\n",
    "        for word, i in word_index.items():\n",
    "            ggle_word_index[clean_numbers(word)] = i\n",
    "    else:\n",
    "        ggle_word_index = word_index\n",
    "\n",
    "    embedding_matrix_1, u1 = load_glove(word_index, max_features, unk_uni)\n",
    "    embedding_matrix_2, u2 = load_wiki(word_index, max_features, unk_uni)\n",
    "    embedding_matrix_3, u3 = load_parag(word_index, max_features, unk_uni)\n",
    "    embedding_matrix_4, u4 = load_ggle(ggle_word_index, max_features, unk_uni)\n",
    "#     embedding_matrix = np.concatenate((embedding_matrix_1,\n",
    "#                                        embedding_matrix_2,\n",
    "#                                        embedding_matrix_3,\n",
    "#                                        embedding_matrix_4), axis=1)\n",
    "    embedding_matrix = np.mean([embedding_matrix_1, embedding_matrix_2,embedding_matrix_3,embedding_matrix_4], axis=0)\n",
    "    del embedding_matrix_1, embedding_matrix_2, embedding_matrix_3, embedding_matrix_4\n",
    "    gc.collect()\n",
    "    # with open('unknowns.pkl', 'wb') as f:\n",
    "    #     pickle.dump({'glove': u1, 'wiki': u2, 'parag': u3, 'ggle': u4}, f)\n",
    "    # print('Embedding:', embedding_matrix.shape)\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total unknowns glove 29120\n",
      "[('quorans', 1916), ('brexit', 2861), ('cryptocurrencies', 2985), ('redmi', 3678), ('paytm', 3895), ('kvpy', 3896), ('iiser', 3973), ('ethereum', 4056), ('iisc', 4626), ('jinping', 5607)]\n",
      "\n",
      "Total unknowns wiki 46137\n",
      "[('upsc', 798), ('aiims', 1603), ('cgl', 1784), ('quorans', 1916), ('jio', 2101), ('manipal', 2170), ('icse', 2384), ('iiit', 2690), ('bitsat', 2692), ('cgpa', 2698)]\n",
      "\n",
      "Total unknowns parag 18344\n",
      "[('quorans', 1916), ('brexit', 2861), ('cryptocurrencies', 2985), ('redmi', 3678), ('₹', 6013), ('coinbase', 7079), ('oneplus', 7430), ('uceed', 8012), ('bhakts', 8442), ('demonetisation', 8447)]\n",
      "\n",
      "Total unknowns ggle 55922\n",
      "[('a', 4), ('to', 5), ('of', 7), ('and', 10), (\"'\", 17), ('’', 81), ('quora', 109), ('”', 433), ('“', 438), ('instagram', 702)]\n",
      "Embedding matrix shape =  (120000, 300)\n"
     ]
    }
   ],
   "source": [
    "#load all of the embeddings file\n",
    "embedding_matrix = load_all_embeddings(word_index, max_features=max_features,\n",
    "                                           clean_num=2, unk_uni=True)\n",
    "print(\"Embedding matrix shape = \",np.shape(embedding_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, feature_dim, step_dim, bias=True, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "        \n",
    "        self.supports_masking = True\n",
    "\n",
    "        self.bias = bias\n",
    "        self.feature_dim = feature_dim\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        \n",
    "        weight = torch.zeros(feature_dim, 1)\n",
    "        nn.init.xavier_uniform_(weight)\n",
    "        self.weight = nn.Parameter(weight)\n",
    "        \n",
    "        if bias:\n",
    "            self.b = nn.Parameter(torch.zeros(step_dim))\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        feature_dim = self.feature_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = torch.mm(\n",
    "            x.contiguous().view(-1, feature_dim), \n",
    "            self.weight\n",
    "        ).view(-1, step_dim)\n",
    "        \n",
    "        if self.bias:\n",
    "            eij = eij + self.b\n",
    "            \n",
    "        eij = torch.tanh(eij)\n",
    "        a = torch.exp(eij)\n",
    "        \n",
    "        if mask is not None:\n",
    "            a = a * mask\n",
    "\n",
    "        a = a / torch.sum(a, 1, keepdim=True) + 1e-10\n",
    "\n",
    "        weighted_input = x * torch.unsqueeze(a, -1)\n",
    "        return torch.sum(weighted_input, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        \n",
    "        hidden_size = 60\n",
    "        \n",
    "        self.embedding = nn.Embedding(max_features, embed_size)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        self.embedding_dropout = nn.Dropout2d(0.1)\n",
    "        self.lstm = nn.GRU(embed_size, hidden_size, bidirectional=True, batch_first=True)\n",
    "        self.gru = nn.GRU(hidden_size*2, hidden_size, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        self.lstm_attention = Attention(hidden_size*2, maxlen)\n",
    "        self.gru_attention = Attention(hidden_size*2, maxlen)\n",
    "        \n",
    "        self.linear = nn.Linear(480, 16)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.out = nn.Linear(16, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h_embedding = self.embedding(x)\n",
    "        h_embedding = torch.squeeze(self.embedding_dropout(torch.unsqueeze(h_embedding, 0)))\n",
    "        \n",
    "        h_lstm, _ = self.lstm(h_embedding) #h_lstm output state(seq_len, batch, hidden_size * num_directions), _ is hidden state (num_layers * num_directions, batch, hidden_size)\n",
    "        h_gru, _ = self.gru(h_lstm)\n",
    "        \n",
    "        h_lstm_atten = self.lstm_attention(h_lstm)\n",
    "        h_gru_atten = self.gru_attention(h_gru)\n",
    "        \n",
    "        avg_pool = torch.mean(h_gru, 1)\n",
    "        max_pool, _ = torch.max(h_gru, 1)\n",
    "        \n",
    "        conc = torch.cat((h_lstm_atten, h_gru_atten, avg_pool, max_pool), 1)\n",
    "#         print(conc.size(),h_gru_atten.size(),h_lstm_atten.size(),avg_pool.size(),max_pool.size())\n",
    "        conc = self.relu(self.linear(conc))\n",
    "        conc = self.dropout(conc)\n",
    "        out = self.out(conc)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNet(\n",
       "  (embedding): Embedding(120000, 300)\n",
       "  (embedding_dropout): Dropout2d(p=0.1, inplace=False)\n",
       "  (lstm): GRU(300, 60, batch_first=True, bidirectional=True)\n",
       "  (gru): GRU(120, 60, batch_first=True, bidirectional=True)\n",
       "  (lstm_attention): Attention()\n",
       "  (gru_attention): Attention()\n",
       "  (linear): Linear(in_features=480, out_features=16, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (out): Linear(in_features=16, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load model\n",
    "m = NeuralNet()\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#used 5 stratified Kfold\n",
    "splits = list(StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED).split(train_X, train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "F1 Score is an evaluation metric ranging from 0 to 1.As data is not balancee we use it instead of accuracy \n",
    "because it is immune to class imabalance.\n",
    "'''\n",
    "def threshold_search(y_true, y_proba):\n",
    "    best_threshold = 0\n",
    "    best_score = 0\n",
    "    for threshold in tqdm([i * 0.01 for i in range(100)]):\n",
    "        score = f1_score(y_true=y_true, y_pred=y_proba > threshold)\n",
    "        if score > best_score:\n",
    "            best_threshold = threshold\n",
    "            best_score = score\n",
    "    search_result = {'threshold': best_threshold, 'f1': best_score}\n",
    "    return search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Epoch 1/8 \t loss=222.3383 \t val_loss=170.3929 \t time=52.92s\n",
      "Epoch 2/8 \t loss=185.8844 \t val_loss=164.5262 \t time=52.20s\n",
      "Epoch 3/8 \t loss=178.2769 \t val_loss=159.7338 \t time=51.88s\n",
      "Epoch 4/8 \t loss=172.9873 \t val_loss=155.0276 \t time=51.96s\n",
      "Epoch 5/8 \t loss=169.1411 \t val_loss=152.2564 \t time=51.98s\n",
      "Epoch 6/8 \t loss=165.3235 \t val_loss=158.9538 \t time=51.42s\n",
      "Epoch 7/8 \t loss=161.1196 \t val_loss=151.2997 \t time=50.96s\n",
      "Epoch 8/8 \t loss=157.5097 \t val_loss=157.2959 \t time=51.84s\n",
      "Fold 2\n",
      "Epoch 1/8 \t loss=224.8337 \t val_loss=177.0869 \t time=51.62s\n",
      "Epoch 2/8 \t loss=186.5286 \t val_loss=161.5248 \t time=52.07s\n",
      "Epoch 3/8 \t loss=179.3119 \t val_loss=163.6575 \t time=52.77s\n",
      "Epoch 4/8 \t loss=172.8003 \t val_loss=153.9992 \t time=52.79s\n",
      "Epoch 5/8 \t loss=168.7780 \t val_loss=154.0880 \t time=51.71s\n",
      "Epoch 6/8 \t loss=164.6151 \t val_loss=153.6308 \t time=52.29s\n",
      "Epoch 7/8 \t loss=161.3764 \t val_loss=158.4761 \t time=51.76s\n",
      "Epoch 8/8 \t loss=157.4690 \t val_loss=155.7132 \t time=52.59s\n",
      "Fold 3\n",
      "Epoch 1/8 \t loss=237.1232 \t val_loss=171.3244 \t time=50.84s\n",
      "Epoch 2/8 \t loss=189.2603 \t val_loss=161.7627 \t time=51.97s\n",
      "Epoch 3/8 \t loss=181.2609 \t val_loss=166.7465 \t time=52.19s\n",
      "Epoch 4/8 \t loss=176.6422 \t val_loss=166.1030 \t time=51.88s\n",
      "Epoch 5/8 \t loss=171.9211 \t val_loss=160.6386 \t time=51.52s\n",
      "Epoch 6/8 \t loss=167.2502 \t val_loss=157.1071 \t time=52.87s\n",
      "Epoch 7/8 \t loss=163.1119 \t val_loss=152.1302 \t time=52.64s\n",
      "Epoch 8/8 \t loss=159.9067 \t val_loss=161.2917 \t time=51.60s\n",
      "Fold 4\n",
      "Epoch 1/8 \t loss=228.2270 \t val_loss=172.4498 \t time=51.54s\n",
      "Epoch 2/8 \t loss=189.0996 \t val_loss=162.2729 \t time=53.06s\n",
      "Epoch 3/8 \t loss=181.9717 \t val_loss=161.9797 \t time=52.44s\n",
      "Epoch 4/8 \t loss=176.5655 \t val_loss=154.8080 \t time=52.42s\n",
      "Epoch 5/8 \t loss=171.7045 \t val_loss=152.6811 \t time=51.66s\n",
      "Epoch 6/8 \t loss=167.7119 \t val_loss=153.9443 \t time=51.91s\n",
      "Epoch 7/8 \t loss=163.7146 \t val_loss=152.4789 \t time=53.20s\n",
      "Epoch 8/8 \t loss=159.5569 \t val_loss=155.6814 \t time=52.43s\n",
      "Fold 5\n",
      "Epoch 1/8 \t loss=231.7789 \t val_loss=171.1258 \t time=52.24s\n",
      "Epoch 2/8 \t loss=189.2932 \t val_loss=163.4103 \t time=51.66s\n",
      "Epoch 3/8 \t loss=180.4031 \t val_loss=165.0451 \t time=51.17s\n",
      "Epoch 4/8 \t loss=175.9622 \t val_loss=158.7950 \t time=53.28s\n",
      "Epoch 5/8 \t loss=171.1360 \t val_loss=158.8077 \t time=52.65s\n",
      "Epoch 6/8 \t loss=166.9937 \t val_loss=152.5035 \t time=52.66s\n",
      "Epoch 7/8 \t loss=163.1057 \t val_loss=152.9676 \t time=51.78s\n",
      "Epoch 8/8 \t loss=159.2156 \t val_loss=153.0016 \t time=51.73s\n"
     ]
    }
   ],
   "source": [
    "train_preds = np.zeros((len(train_X)))\n",
    "test_preds = np.zeros((len(test_X)))\n",
    "\n",
    "seed_torch(SEED)\n",
    "\n",
    "x_test_cuda = torch.tensor(test_X, dtype=torch.long).cuda()\n",
    "test = torch.utils.data.TensorDataset(x_test_cuda)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "for i, (train_idx, valid_idx) in enumerate(splits):\n",
    "    x_train_fold = torch.tensor(train_X[train_idx], dtype=torch.long).cuda()\n",
    "    y_train_fold = torch.tensor(train_y[train_idx, np.newaxis], dtype=torch.float32).cuda()\n",
    "    x_val_fold = torch.tensor(train_X[valid_idx], dtype=torch.long).cuda()\n",
    "    y_val_fold = torch.tensor(train_y[valid_idx, np.newaxis], dtype=torch.float32).cuda()\n",
    "    \n",
    "    model = NeuralNet()\n",
    "    model.cuda()\n",
    "    \n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss(reduction=\"sum\")\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    \n",
    "    train = torch.utils.data.TensorDataset(x_train_fold, y_train_fold)\n",
    "    valid = torch.utils.data.TensorDataset(x_val_fold, y_val_fold)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    print(f'Fold {i + 1}')\n",
    "    \n",
    "    for epoch in range(train_epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        model.train()\n",
    "        avg_loss = 0.\n",
    "        for x_batch, y_batch in tqdm(train_loader, disable=True):\n",
    "            y_pred = model(x_batch)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item() / len(train_loader)\n",
    "        \n",
    "        model.eval()\n",
    "        valid_preds_fold = np.zeros((x_val_fold.size(0)))\n",
    "        test_preds_fold = np.zeros(len(test_X))\n",
    "        avg_val_loss = 0.\n",
    "        for i, (x_batch, y_batch) in enumerate(valid_loader):\n",
    "            y_pred = model(x_batch).detach()\n",
    "            avg_val_loss += loss_fn(y_pred, y_batch).item() / len(valid_loader)\n",
    "            valid_preds_fold[i * batch_size:(i+1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n",
    "        \n",
    "        elapsed_time = time.time() - start_time \n",
    "        print('Epoch {}/{} \\t loss={:.4f} \\t val_loss={:.4f} \\t time={:.2f}s'.format(\n",
    "            epoch + 1, train_epochs, avg_loss, avg_val_loss, elapsed_time))\n",
    "        \n",
    "    for i, (x_batch,) in enumerate(test_loader):\n",
    "        y_pred = model(x_batch).detach()\n",
    "        \n",
    "        test_preds_fold[i * batch_size:(i+1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n",
    "\n",
    "    train_preds[valid_idx] = valid_preds_fold\n",
    "    test_preds += test_preds_fold / len(splits)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:43<00:00,  2.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'threshold': 0.24, 'f1': 0.6780404106483952}\n"
     ]
    }
   ],
   "source": [
    "search_result = threshold_search(train_y, train_preds)\n",
    "print(search_result)\n",
    "sub = pd.read_csv('../input/quora-insincere-questions-classification/sample_submission.csv')\n",
    "sub.prediction = test_preds > search_result['threshold']\n",
    "sub.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
